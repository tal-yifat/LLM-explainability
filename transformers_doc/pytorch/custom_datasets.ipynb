{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_datasets.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tal-yifat/lost-work-text-analysis/blob/main/transformers_doc/pytorch/custom_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCf2keCyGXak",
        "outputId": "ff276698-21d7-42d7-9e25-96a2fd2867be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 4.1 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290 kB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 17.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3 MB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 36.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125 kB 48.1 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243 kB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 271 kB 52.4 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: multidict, yarl, async-timeout, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.14.0 fsspec-2021.10.1 huggingface-hub-0.0.19 multidict-5.2.0 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3 xxhash-2.0.2 yarl-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmcSf6SYGXau"
      },
      "source": [
        "# Fine-tuning with custom datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOjCKx9jGXa2"
      },
      "source": [
        "> **NOTE:** The datasets used in this tutorial are available and can be more easily accessed using the [ðŸ¤— Datasets library](https://github.com/huggingface/datasets). We do not use this library to access the datasets here since this\n",
        "> tutorial meant to illustrate how to work with your own data. A brief of introduction can be found at the end of the\n",
        "> tutorial in the section \"[datasetslib](#datasetslib)\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp1GOJdcGXa7"
      },
      "source": [
        "This tutorial will take you through several examples of using ðŸ¤— Transformers models with your own datasets. The guide\n",
        "shows one of many valid workflows for using these models and is meant to be illustrative rather than definitive. We\n",
        "show examples of reading in several data formats, preprocessing the data for several types of tasks, and then preparing\n",
        "the data into PyTorch/TensorFlow `Dataset` objects which can easily be used either with\n",
        "`Trainer`/`TFTrainer` or with native PyTorch/TensorFlow.\n",
        "\n",
        "We include several examples, each of which demonstrates a different type of common downstream task:\n",
        "\n",
        "  - [seq_imdb](#seq_imdb)\n",
        "  - [tok_ner](#tok_ner)\n",
        "  - [qa_squad](#qa_squad)\n",
        "  - [resources](#resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZktIYbgWGXbC"
      },
      "source": [
        "<a id='seq_imdb'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lF_fmbMGXbG"
      },
      "source": [
        "## Sequence Classification with IMDb Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zaLGqFLGXbK"
      },
      "source": [
        "> **NOTE:** This dataset can be explored in the Hugging Face model hub ([IMDb](https://huggingface.co/datasets/imdb)), and\n",
        "> can be alternatively downloaded with the ðŸ¤— Datasets library with `load_dataset(\"imdb\")`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVN621NoGXbR"
      },
      "source": [
        "In this example, we'll show how to download, tokenize, and train a model on the IMDb reviews dataset. This task takes\n",
        "the text of a review and requires the model to predict whether the sentiment of the review is positive or negative.\n",
        "Let's start by downloading the dataset from the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) webpage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZNcXY4sGXbX"
      },
      "source": [
        "! wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "! tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQW-4nT0GXbk"
      },
      "source": [
        "This data is organized into `pos` and `neg` folders with one text file per example. Let's write a function that can\n",
        "read this in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElbJTD5xGXbo"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def read_imdb_split(split_dir):\n",
        "    split_dir = Path(split_dir)\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for label_dir in [\"pos\", \"neg\"]:\n",
        "        for text_file in (split_dir/label_dir).iterdir():\n",
        "            texts.append(text_file.read_text())\n",
        "            labels.append(0 if label_dir is \"neg\" else 1)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
        "test_texts, test_labels = read_imdb_split('aclImdb/test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g84rQhr_GXbr"
      },
      "source": [
        "We now have a train and test dataset, but let's also also create a validation set which we can use for for evaluation\n",
        "and tuning without tainting our test set results. Sklearn has a convenient utility for creating such splits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDlpsk0kGXbu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEa7zXIbGXbx"
      },
      "source": [
        "Alright, we've read in our dataset. Now let's tackle tokenization. We'll eventually train a classifier using\n",
        "pre-trained DistilBert, so let's use the DistilBert tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0QdkQ8MGXbz"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sDKP6WRGXb0"
      },
      "source": [
        "Now we can simply pass our texts to the tokenizer. We'll pass `truncation=True` and `padding=True`, which will\n",
        "ensure that all of our sequences are padded to the same length and are truncated to be no longer model's maximum input\n",
        "length. This will allow us to feed batches of sequences into the model at the same time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvrk0z-QGXb2"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6MgslQMGXb2"
      },
      "source": [
        "Now, let's turn our labels and encodings into a Dataset object. In PyTorch, this is done by subclassing a\n",
        "`torch.utils.data.Dataset` object and implementing `__len__` and `__getitem__`. In TensorFlow, we pass our input\n",
        "encodings and labels to the `from_tensor_slices` constructor method. We put the data in this format so that the data\n",
        "can be easily batched such that each key in the batch encoding corresponds to a named parameter of the\n",
        "`DistilBertForSequenceClassification.forward` method of the model we will train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkDTT7f1GXb4"
      },
      "source": [
        "import torch\n",
        "\n",
        "class IMDbDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
        "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
        "test_dataset = IMDbDataset(test_encodings, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-O1VClZGXb5"
      },
      "source": [
        "Now that our datasets our ready, we can fine-tune a model either with the ðŸ¤—\n",
        "`Trainer`/`TFTrainer` or with native PyTorch/TensorFlow. See [training](https://huggingface.co/transformers/training.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXWJwALvGXb7"
      },
      "source": [
        "<a id='ft_trainer'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF6maUZWGXb7"
      },
      "source": [
        "### Fine-tuning with Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBgDeoeMGXb7"
      },
      "source": [
        "The steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model\n",
        "to fine-tune, define the `TrainingArguments`/`TFTrainingArguments` and\n",
        "instantiate a `Trainer`/`TFTrainer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBFRF4XuGXb-"
      },
      "source": [
        "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwRx_jqJGXb-"
      },
      "source": [
        "<a id='ft_native'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYg8aznQGXb-"
      },
      "source": [
        "### Fine-tuning with native PyTorch/TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKH4A7TeGXb-"
      },
      "source": [
        "We can also train use native PyTorch or TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbRpmtP_GXb-"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DistilBertForSequenceClassification, AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDpYu109GXcA"
      },
      "source": [
        "<a id='tok_ner'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDP516MPGXcB"
      },
      "source": [
        "## Token Classification with W-NUT Emerging Entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZVWpP9aGXcB"
      },
      "source": [
        "> **NOTE:** This dataset can be explored in the Hugging Face model hub ([WNUT-17](https://huggingface.co/datasets/wnut_17)),\n",
        "> and can be alternatively downloaded with the ðŸ¤— Datasets library with `load_dataset(\"wnut_17\")`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpIeKsr1GXcB"
      },
      "source": [
        "Next we will look at token classification. Rather than classifying an entire sequence, this task classifies token by\n",
        "token. We'll demonstrate how to do this with [Named Entity Recognition](http://nlpprogress.com/english/named_entity_recognition.html), which involves identifying tokens which correspond to\n",
        "a predefined set of \"entities\". Specifically, we'll use the [W-NUT Emerging and Rare entities](http://noisy-text.github.io/2017/emerging-rare-entities.html) corpus. The data is given as a collection of\n",
        "pre-tokenized documents where each token is assigned a tag.\n",
        "\n",
        "Let's start by downloading the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKEoOjSzGXcB"
      },
      "source": [
        "! wget http://noisy-text.github.io/2017/files/wnut17train.conll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_6lljKcGXcC"
      },
      "source": [
        "In this case, we'll just download the train set, which is a single text file. Each line of the file contains either (1)\n",
        "a word and tag separated by a tab, or (2) a blank line indicating the end of a document. Let's write a function to read\n",
        "this in. We'll take in the file path and return `token_docs` which is a list of lists of token strings, and\n",
        "`token_tags` which is a list of lists of tag strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BD0CH4zGXcC"
      },
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "def read_wnut(file_path):\n",
        "    file_path = Path(file_path)\n",
        "\n",
        "    raw_text = file_path.read_text().strip()\n",
        "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for doc in raw_docs:\n",
        "        tokens = []\n",
        "        tags = []\n",
        "        for line in doc.split('\\n'):\n",
        "            token, tag = line.split('\\t')\n",
        "            tokens.append(token)\n",
        "            tags.append(tag)\n",
        "        token_docs.append(tokens)\n",
        "        tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs\n",
        "\n",
        "texts, tags = read_wnut('wnut17train.conll')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB2Z7DE8GXcC"
      },
      "source": [
        "Just to see what this data looks like, let's take a look at a segment of the first document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVFgjNm7GXcD",
        "outputId": "f3179dab-df3a-4fb1-bb3c-0c53e567730f"
      },
      "source": [
        "print(texts[0][10:17], tags[0][10:17], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n",
              "['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsaSMoYMGXcE"
      },
      "source": [
        "`location` is an entity type, `B-` indicates the beginning of an entity, and `I-` indicates consecutive positions\n",
        "of the same entity (\"Empire State Building\" is considered one entity). `O` indicates the token does not correspond to\n",
        "any entity.\n",
        "\n",
        "Now that we've read the data in, let's create a train/validation split:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0X3CDPvGXcF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rz15O3GGXcF"
      },
      "source": [
        "Next, let's create encodings for our tokens and tags. For the tags, we can start by just create a simple mapping which\n",
        "we'll use in a moment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoxFP6SZGXcF"
      },
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laieys3BGXcG"
      },
      "source": [
        "To encode the tokens, we'll use a pre-trained DistilBert tokenizer. We can tell the tokenizer that we're dealing with\n",
        "ready-split tokens rather than full sentence strings by passing `is_split_into_words=True`. We'll also pass\n",
        "`padding=True` and `truncation=True` to pad the sequences to be the same length. Lastly, we can tell the model to\n",
        "return information about the tokens which are split by the wordpiece tokenization process, which we will need in a\n",
        "moment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezz2iONbGXcG"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAbbpgziGXcG"
      },
      "source": [
        "Great, so now our tokens are nicely encoded in the format that they need to be in to feed them into our DistilBert\n",
        "model below.\n",
        "\n",
        "Now we arrive at a common obstacle with using pre-trained models for token-level classification: many of the tokens in\n",
        "the W-NUT corpus are not in DistilBert's vocabulary. Bert and many models like it use a method called WordPiece\n",
        "Tokenization, meaning that single words are split into multiple tokens such that each token is likely to be in the\n",
        "vocabulary. For example, DistilBert's tokenizer would split the Twitter handle `@huggingface` into the tokens `['@',\n",
        "'hugging', '##face']`. This is a problem for us because we have exactly one tag per token. If the tokenizer splits a\n",
        "token into multiple sub-tokens, then we will end up with a mismatch between our tokens and our labels.\n",
        "\n",
        "One way to handle this is to only train on the tag labels for the first subtoken of a split token. We can do this in ðŸ¤—\n",
        "Transformers by setting the labels we wish to ignore to `-100`. In the example above, if the label for\n",
        "`@HuggingFace` is `3` (indexing `B-corporation`), we would set the labels of `['@', 'hugging', '##face']` to\n",
        "`[3, -100, -100]`.\n",
        "\n",
        "Let's write a function to do this. This is where we will use the `offset_mapping` from the tokenizer as mentioned\n",
        "above. For each sub-token returned by the tokenizer, the offset mapping gives us a tuple indicating the sub-token's\n",
        "start position and end position relative to the original token it was split from. That means that if the first position\n",
        "in the tuple is anything other than `0`, we will set its corresponding label to `-100`. While we're at it, we can\n",
        "also set labels to `-100` if the second position of the offset mapping is `0`, since this means it must be a\n",
        "special token like `[PAD]` or `[CLS]`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTjRNa5_GXcG"
      },
      "source": [
        "> **NOTE:** Due to a recently fixed bug, -1 must be used instead of -100 when using TensorFlow in ðŸ¤— Transformers <= 3.02."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAnOtqgeGXcG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_tags(tags, encodings):\n",
        "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        # create an empty array of -100\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        # set labels whose first offset position is 0 and the second is not 0\n",
        "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "        encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "    return encoded_labels\n",
        "\n",
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juBTL0hAGXcH"
      },
      "source": [
        "The hard part is now done. Just as in the sequence classification example above, we can create a dataset object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYJuPnybGXcH"
      },
      "source": [
        "import torch\n",
        "\n",
        "class WNUTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
        "val_dataset = WNUTDataset(val_encodings, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCzvoRBUGXcK"
      },
      "source": [
        "Now load in a token classification model and specify the number of labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHZlQcKlGXcK"
      },
      "source": [
        "from transformers import DistilBertForTokenClassification\n",
        "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKlLNJElGXcK"
      },
      "source": [
        "The data and model are both ready to go. You can train the model either with\n",
        "`Trainer`/`TFTrainer` or with native PyTorch/TensorFlow, exactly as in the\n",
        "sequence classification example above.\n",
        "\n",
        "  - [ft_trainer](#ft_trainer)\n",
        "  - [ft_native](#ft_native)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNQkQuRwGXcK"
      },
      "source": [
        "<a id='qa_squad'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH0qt5sWGXcK"
      },
      "source": [
        "## Question Answering with SQuAD 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbTClXz8GXcK"
      },
      "source": [
        "> **NOTE:** This dataset can be explored in the Hugging Face model hub ([SQuAD V2](https://huggingface.co/datasets/squad_v2)), and can be alternatively downloaded with the ðŸ¤— Datasets library with\n",
        "> `load_dataset(\"squad_v2\")`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Wf0MjuGXcK"
      },
      "source": [
        "Question answering comes in many forms. In this example, we'll look at the particular type of extractive QA that\n",
        "involves answering a question about a passage by highlighting the segment of the passage that answers the question.\n",
        "This involves fine-tuning a model which predicts a start position and an end position in the passage. We will use the\n",
        "[Stanford Question Answering Dataset (SQuAD) 2.0](https://rajpurkar.github.io/SQuAD-explorer/).\n",
        "\n",
        "We will start by downloading the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia5GpdiRGXcL"
      },
      "source": [
        "! mkdir squad\n",
        "! wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\n",
        "! wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Waod28LhGXcL"
      },
      "source": [
        "Each split is in a structured json file with a number of questions and answers for each passage (or context). We'll\n",
        "take this apart into parallel lists of contexts, questions, and answers (note that the contexts here are repeated since\n",
        "there are multiple questions per context):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeAz79OkGXcN"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                for answer in qa['answers']:\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "\n",
        "    return contexts, questions, answers\n",
        "\n",
        "train_contexts, train_questions, train_answers = read_squad('squad/train-v2.0.json')\n",
        "val_contexts, val_questions, val_answers = read_squad('squad/dev-v2.0.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiGktAJhGXcN"
      },
      "source": [
        "The contexts and questions are just strings. The answers are dicts containing the subsequence of the passage with the\n",
        "correct answer as well as an integer indicating the character at which the answer begins. In order to train a model on\n",
        "this data we need (1) the tokenized context/question pairs, and (2) integers indicating at which **token** positions the\n",
        "answer begins and ends.\n",
        "\n",
        "First, let's get the **character** position at which the answer ends in the passage (we are given the starting position).\n",
        "Sometimes SQuAD answers are off by one or two characters, so we will also adjust for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDqA2SVNGXcO"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # sometimes squad answers are off by a character or two â€“ fix this\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoGqfh6bGXcO"
      },
      "source": [
        "Now `train_answers` and `val_answers` include the character end positions and the corrected start positions. Next,\n",
        "let's tokenize our context/question pairs. ðŸ¤— Tokenizers can accept parallel lists of sequences and encode them together\n",
        "as sequence pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGs5T-AbGXcO"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5IGxPrsGXcO"
      },
      "source": [
        "Next we need to convert our character start/end positions to token start/end positions. When using ðŸ¤— Fast Tokenizers,\n",
        "we can use the built in `BatchEncoding.char_to_token` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5_xLEo4GXcO"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFwqCdXoGXcP"
      },
      "source": [
        "Our data is ready. Let's just put it in a PyTorch/TensorFlow dataset so that we can easily use it for training. In\n",
        "PyTorch, we define a custom `Dataset` class. In TensorFlow, we pass a tuple of `(inputs_dict, labels_dict)` to the\n",
        "`from_tensor_slices` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyJ5hznIGXcP"
      },
      "source": [
        "import torch\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJQfU8-zGXcP"
      },
      "source": [
        "Now we can use a DistilBert model with a QA head for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6Rz6RM3GXcP"
      },
      "source": [
        "from transformers import DistilBertForQuestionAnswering\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFOIAdc4GXcR"
      },
      "source": [
        "The data and model are both ready to go. You can train the model with\n",
        "`Trainer`/`TFTrainer` exactly as in the sequence classification example\n",
        "above. If using native PyTorch, replace `labels` with `start_positions` and `end_positions` in the training\n",
        "example. If using Keras's `fit`, we need to make a minor modification to handle this example since it involves\n",
        "multiple model outputs.\n",
        "\n",
        "  - [ft_trainer](#ft_trainer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HigtzwuSGXcR"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZmidfCvGXcR"
      },
      "source": [
        "<a id='resources'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH7VL0DHGXcR"
      },
      "source": [
        "## Additional Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks4BOwIgGXcS"
      },
      "source": [
        "  - [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train). Blog post showing the steps to load in Esperanto data and train a\n",
        "    masked language model from scratch.\n",
        "  - [Preprocessing](https://huggingface.co/transformers/preprocessing.html). Docs page on data preprocessing.\n",
        "  - [Training](https://huggingface.co/transformers/training.html). Docs page on training and fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylxe_8EnGXcS"
      },
      "source": [
        "<a id='datasetslib'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drv5msl9GXcS"
      },
      "source": [
        "### Using the ðŸ¤— Datasets & Metrics library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvFuyd1FGXcS"
      },
      "source": [
        "This tutorial demonstrates how to read in datasets from various raw text formats and prepare them for training with ðŸ¤—\n",
        "Transformers so that you can do the same thing with your own custom datasets. However, we recommend users use the [ðŸ¤—\n",
        "Datasets library](https://github.com/huggingface/datasets) for working with the 150+ datasets included in the [hub](https://huggingface.co/datasets), including the three datasets used in this tutorial. As a very brief overview, we\n",
        "will show how to use the Datasets library to download and prepare the IMDb dataset from the first example,\n",
        "[seq_imdb](#seq_imdb).\n",
        "\n",
        "Start by downloading the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWgW6b8UGXcS"
      },
      "source": [
        "from datasets import load_dataset\n",
        "train = load_dataset(\"imdb\", split=\"train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPXTzZFnGXcS"
      },
      "source": [
        "Each dataset has multiple columns corresponding to different features. Let's see what our columns are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLOV2xhGXcS",
        "outputId": "db780b02-07f2-4206-ddf8-afebc230f14e"
      },
      "source": [
        "print(train.column_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['label', 'text']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7M0-mt-GXcT"
      },
      "source": [
        "Great. Now let's tokenize the text. We can do this using the `map` method. We'll also rename the `label` column to\n",
        "`labels` to match the model's input arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK6V06iNGXcT"
      },
      "source": [
        "train = train.map(lambda batch: tokenizer(batch[\"text\"], truncation=True, padding=True), batched=True)\n",
        "train.rename_column_(\"label\", \"labels\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d5Hd0zbGXcT"
      },
      "source": [
        "Lastly, we can use the `set_format` method to determine which columns and in what data format we want to access\n",
        "dataset elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtkmYm1kGXcT",
        "outputId": "80526ebd-3aba-453b-c894-527397f9f644"
      },
      "source": [
        "train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "{key: val.shape for key, val in train[0].items()})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'labels': torch.Size([]), 'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512])}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wD5cBDwGXcT"
      },
      "source": [
        "We now have a fully-prepared dataset. Check out [the ðŸ¤— Datasets docs](https://huggingface.co/docs/datasets/processing.html) for a more thorough introduction."
      ]
    }
  ]
}